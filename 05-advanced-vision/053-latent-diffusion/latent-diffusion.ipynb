{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Diffusion\n",
    "\n",
    "### Quick Recap on Diffusion Models: \n",
    "\n",
    "Diffusion Models have a Forward Process, where small amount of noise is added to an image $\\mathbf{x}_0$ over multiple steps. The expression demonstrating this is as follows, \n",
    "\n",
    "$$q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) = \\prod_{t=1}^T q(\\mathbf{x_{t}}|\\mathbf{x_{t-1}})$$\n",
    "\n",
    "To explain this process simply, say you start off with a picture of an aircraft, $\\mathbf{x_0}$ at $\\mathbf{t=0}$. In the forward process, noise sampled from a Gaussian Distribution is added to $\\mathbf{x_0}$ at $\\mathbf{t=1}$ to form $\\mathbf{x_1}$. Over large number of iterations of this process, noise accumulates leading to the image turning into an [Isotropic Gaussian Distribution](https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic). In order to do this, there is a scheduler, $\\beta$ which decides how much noise to add at each step. \n",
    "\n",
    "Then in a Backward Process, given as follows, \n",
    "\n",
    "$$p_\\theta(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T)\\prod_{t=1}^T p_\\theta(\\mathbf{x}_{t-1}|\\mathbf{x}_t) \\quad p_\\theta(\\mathbf{x}_{t-1}|\\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t,t), \\mathbf{\\Sigma}_\\theta(\\mathbf{x}_t,t))$$\n",
    "\n",
    "Where a model is trained to reverse the backward process, i.e, given a noisy image at $\\mathbf{t}$, predict the image that occured in step $\\mathbf{t-1}$. If the model converges, then we can generate an image starting from random noise, by denoising it step-by-step. \n",
    "\n",
    "### Training\n",
    "\n",
    "To train the model, we take an image, $\\mathbf{x}_0$, a random noise sample $\\epsilon$, and a time-step $\\mathbf{t}$. Using the properties of the forward process, we can compute the result of the $\\mathbf{t}^{th}$ iteration straight away using the following equation: \n",
    "\n",
    "$$\\mathbf{x}_t = \\sqrt{\\bar\\alpha_t} \\mathbf{x}_0 + \\sqrt{1-\\bar\\alpha_t} \\epsilon \\;\\;\\;\\;\\;\\; \\bar\\alpha_t = \\prod_{i=1}^{i=t} \\alpha_i $$\n",
    "\n",
    "The model, an U-net architecture with t-embedded in certain steps, is then trained such that given a noisy image, $\\mathbf{x}_t$ it predicts the noise added to the original high-resolution image, $\\epsilon$. This predicted noise is denoted as $\\epsilon_0$ and the model is trained using the MSE $||\\epsilon - \\epsilon_0||_{2}^{2}$ as loss function.\n",
    "\n",
    "## Why do we need Latent Diffusion Models?\n",
    "\n",
    "DDPMs are good for small images, but as their sizes are increased the compute needed to train increases exponentially. For example, it took **8 V100 GPUs** over 11 hours to train the DDPM on the CIFAR dataset. This demands something more efficient to train larger resolution images on a single GPU. [Rombach et al](https://github.com/CompVis/latent-diffusion) proposed Latent Diffusion Models where DM's are trained in the latent space of pre-trained auto-encoders instead of the pixel space as used in DDPMs. This helped them increase quality of the images, while reducing training costs. \n",
    "\n",
    "Essentially, LDMs first train an autoencoder. The encoder takes an image and makes a latent space representation of it, which the decoder then converts this latent space image back to the image of the original resolution. The entire autoencoer is trained using either L1/L2 loss. \n",
    "\n",
    "Then a diffusion model is trained to take noise, and produce latent space representations, which the trained decoder then converts to an image of the original resolution. \n",
    "\n",
    "In a VAE, the encoder, given an input image, generates a mean and variance of the encoder distribution of theinput image in the latent space. The decoder then recieves a sample from this distribution and is reponsible for generating the original image. The entire VAE is trained using a combination of reconstruction and KL Divergence loss to ensure that the encoder distributon is as close to the prior normal distribution as possible, for smooth transition in the latent space. \n",
    "\n",
    "When a VAE is trained usng L1/L2 loss, the reconstruction image matches the contents of the original image but the quality of the image is lower and blurry, as high frequency information is lost during reconstruction. Even within images, the MSE has a very low value as the only difference is the blur as the contents of the images are otherwise the same. To solve the issue of L2 or MSE not being able to assess the perceptual difference b/w images, a **Perceptual Loss** term had to be added. \n",
    "\n",
    "## Perceptual Loss: LPIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from collections import namedtuple\n",
    "import os\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg16(nn.Module):\n",
    "    def __init__(self, requires_grad=False, pretrained=True):\n",
    "        super().__init__()\n",
    "        vgg_pretrained_weights = torch.models.vgg16(pretrained=pretrained).features\n",
    "        # Initialise 5 blocks to access intermediate data \n",
    "        self.block1 = nn.Sequential()\n",
    "        self.block2 = nn.Sequential()\n",
    "        self.block3 = nn.Sequential()\n",
    "        self.block4 = nn.Sequential()\n",
    "        self.block5 = nn.Sequential()\n",
    "\n",
    "        # Break the for loop into multiple segments to get intermediate data required for LPIPs\n",
    "        for i in range(4):\n",
    "            self.block1.add_module(str(i), vgg_pretrained_weights[i])\n",
    "        for i in range(4, 9): \n",
    "            self.block2.add_module(str[i], vgg_pretrained_weights[i])\n",
    "        for i in range(9, 16):\n",
    "            self.block3.add_module(str[i], vgg_pretrained_weights[i])\n",
    "        for i in range(16, 23):\n",
    "            self.block4.add_module(str[i], vgg_pretrained_weights[i])\n",
    "        for i in range(23, 30):\n",
    "            self.block5.add_module(str[i], vgg_pretrained_weights[i])\n",
    "\n",
    "        # Freeze the model just in case \n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.block1(x)\n",
    "        hrelu12 = h\n",
    "        h = self.block2(h)\n",
    "        hrelu22 = h\n",
    "        h = self.block3(h)\n",
    "        hrelu33 = h\n",
    "        h = self.block4(h)\n",
    "        hrelu43 = h\n",
    "        h = self.block5(h)\n",
    "        hrelu53 = h\n",
    "        vggout = namedtuple(\"VGG Outputs\", [\"relu12, relu22, relu33, relu43, relu53\"]) # relyxy means the y-th relu of the x-th layer \n",
    "        out = vggout(hrelu12, hrelu22, hrelu33, hrelu43, hrelu53)\n",
    "        return out\n",
    "    \n",
    "class LPIPS(nn.Module):\n",
    "    def __init__(self, net='vgg', version='0.1', use_dropout=True):\n",
    "        super().__init__()\n",
    "        self.version = version\n",
    "        self.scaling_layer = ScalingLayer() # Normalisation\n",
    "        self.chns = [64, 128, 256, 512, 512]\n",
    "        self.L = len(self.chns)\n",
    "        self.net = vgg16(pretrained=True, requires_grad=False)\n",
    "\n",
    "        # Each one corresponds to a different level of VGG features\n",
    "        self.lin0 = NetLinLayer(self.chns[0], use_dropout=use_dropout)\n",
    "        self.lin1 = NetLinLayer(self.chns[1], use_dropout=use_dropout)\n",
    "        self.lin2 = NetLinLayer(self.chns[2], use_dropout=use_dropout)\n",
    "        self.lin3 = NetLinLayer(self.chns[3], use_dropout=use_dropout)\n",
    "        self.lin4 = NetLinLayer(self.chns[4], use_dropout=use_dropout)\n",
    "        self.lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]\n",
    "        self.lins = nn.ModuleList(self.lins)\n",
    "\n",
    "        # Load weights of a trained LPIPS model\n",
    "        model_path = os.path.abspath(os.path.join(inspect.getfile(self.__init__), '..', 'weights/v%s/%s.pth' % (version, net)))\n",
    "        print('Loading model from: %s' % model_path)\n",
    "        self.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n",
    "\n",
    "        # Freeze all parameters\n",
    "        self.eval()\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, img0, img1, normalise=False):\n",
    "        # Scale the inputs to -1 to +1 range if needed\n",
    "        if normalise:\n",
    "            img0 = 2 * img0 - 1\n",
    "            img1 = 2 * img1 - 1\n",
    "        \n",
    "        # Normalize the inputs according to imagenet normalization\n",
    "        img0_norm, img1_norm = self.scaling_layer(img0), self.scaling_layer(img1)\n",
    "\n",
    "        # Get VGG outputs \n",
    "        img0_vgg, img1_vgg = self.net.forward(img0_norm), self.net.forward(img1_norm)\n",
    "        feats0, feats1, diffs = {}, {}, {}\n",
    "\n",
    "        # Compute sq. diff for each layer output\n",
    "\n",
    "        for i in range(self.L): # self.L is a list of channels; for each layer output\n",
    "            feats0[i], feats1[i] = torch.functional.normalised(img0_vgg[i], img1_vgg[i])\n",
    "            diffs[i] = (feats0[i] - feats1[i]) ** 2\n",
    "        \n",
    "        # diffs[kk] has the normalised difference of the feature maps of both images \n",
    "        # self.lins has the NetLinLayers; self.lins[kk] can be used to access the NetLinLayer with the right number of input channels\n",
    "        # self.lins[kk](diffs[kk]) converts multiple channel input to single channel output\n",
    "        # spatial average averages the values of pixels over hxw and outputs a single number \n",
    "        # this is done for each feature map\n",
    "        res = [spatial_average(self.lins[kk](diffs[kk]), keepdim=True) for kk in range(self.L)] \n",
    "        \n",
    "        val = 0\n",
    "        \n",
    "        # Aggregate the results of each layer\n",
    "        for l in range(self.L):\n",
    "            val += res[l]\n",
    "        return val\n",
    "\n",
    "\n",
    "\n",
    "def spatial_average(x, keepdim=True):\n",
    "    return x.mean([2,3], keepdim=keepdim)\n",
    "\n",
    "class ScalingLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Imagnet normalization for (0-1)\n",
    "        # mean = [0.485, 0.456, 0.406]\n",
    "        # std = [0.229, 0.224, 0.225]\n",
    "        self.register_buffer('shift', torch.Tensor([-.030, -.088, -.188])[None, :, None, None])\n",
    "        self.register_buffer('scale', torch.Tensor([.458, .448, .450])[None, :, None, None])\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        return (inp - self.shift) / self.scale\n",
    "\n",
    "class NetLinLayer(nn.Module):    \n",
    "    def __init__(self, chn_in, chn_out=1, use_dropout=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [nn.Dropout(), ] if (use_dropout) else []\n",
    "        layers += [nn.Conv2d(chn_in, chn_out, 1, stride=1, padding=0, bias=False), ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
