{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Overfitting? \n",
    "\n",
    "Overfitting is when the model really closely fits the training data, which prevents it from generalising to testing data. One way of identifying whether a model is overfit is by eyeing the difference between Validation Loss and Training Loss. From when training is begun, both validation and training loss decreases. Beyond a point however, the Training and Validaiton loss diverges. This is when overfitting occurs, as the model gets more accustomed to the trainign data––and hence training loss decreases––but more alienated to the real data––and hence, the validation loss increases. \n",
    "\n",
    "<img src=\"vtloss.jpg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Means High Variance!\n",
    "\n",
    "Variation refers to how sensitive the prediction is to minor changes to the training data. \n",
    "\n",
    "More complex models, with high number of parameters are highly flexible, and hence this causes Overfitting. This is very common in RandomForests and Neural Networks. \n",
    "\n",
    "## Regularisation\n",
    "\n",
    "Regularisation is a way to limit this flexiblity, in hopes of avoiding the overfitting. For neural networks, this means to lower the weights. The weights relate to the 'importance' of a certain neuron. When overfitting occurs, the importance of the output or the input of a neuron is exxagerated. \n",
    "\n",
    "### Ways to do regularisation: \n",
    "\n",
    "- Constraining the model to have less parameters\n",
    "- Adding more information\n",
    "\n",
    "However these cannot always be achieved, for example you might need a more complex model to do a more complicated task, and similarly for a more complicated task that has limited data, new information cannot directly be produced. For imaging tasks, a way to 'produce' new information is by data augmentation where images are stretched, rotated, etc.\n",
    "\n",
    "Other more preferred methods are, L1, L2, and Weight Decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO: Least Absolute Schrinkage and Selection Operator (L1) and Ridge Regression (L2)\n",
    "\n",
    "Find coefficients $\\hat{\\beta}$ that minimizes the penalized sum of squared residuals (RSS) instead of just the MSE.  \n",
    "\n",
    "$$L_1(\\theta) = \\frac{1}{N} \\sum_{i=1}^N Cost(y_i, f_{\\theta}(x_i)) + \\lambda \\sum_{j=1}^p (|\\hat{\\beta_j}|)$$\n",
    "\n",
    "Here n is the total number of training samples, and p is the total number of coefficients of weights of the network. $\\lambda \\sum_{j=1}^p (|\\hat{\\beta_j}|)$ increases when a (1) predictor has a large effect, or if a (2) lot of predictors have a large number of small effects. \n",
    "\n",
    "LASSO is particularly useful for the second reason. When lots of predictors have tiny effects, the model is likely overfit, as it is likely picking up on noise. Random variations which look like a predictive feature because of noise. \n",
    "\n",
    "An example to understand this better is classifying cat or dog images. You don't need to know, for example, all tint of noise, shape of the 5th stripes, and other features to tell a cat apart from a dog. When a model requires more parameters than it should to do a job, it is likely overfit and can only do well on training data. \n",
    "\n",
    "The penalty term $\\lambda \\sum_{j=1}^p (|\\hat{\\beta_j}|)$ gives penalty for having many parameters with tiny effects; where the severity of the penalty is given by the $\\lambda$ term. Severe penalty incentivises the coefficient for these weak predictors to be 0. \n",
    "\n",
    "In essence, the penalty term shrinks weights toward 0. So, LASSO is called a shrinkage method. In the training process, LASSO sets some weights to 0.\n",
    "\n",
    "> If many parameters add tiny contributions to the final outcome of a model, it is likely overfit.\n",
    "\n",
    "$\\lambda$ acts as a fee to be paid for the inclusion of a variable or parameter in the model. If $\\lambda$ is 0, there is no fee for the inclusion of a parameter making is very similar to Least Squares. Also, $\\lambda = \\infty$ would mean no weights end up in the model. \n",
    "\n",
    "So an intermediate value of $\\lambda$ is required for the best resultant model. \n",
    "\n",
    "Similarly L1 or Ridge Regression uses a very similar concept, it tries to minimise the following where each $\\beta$ is squared. \n",
    "\n",
    "$$L_2(\\theta) = \\frac{1}{N} \\sum_{i=1}^N Cost(y_i, f_{\\theta}(x_i)) + \\lambda \\sum_{j=1}^p (|\\hat{\\beta_j}^2|)$$\n",
    "\n",
    "Ridge Regression also pushes variables towards 0, but does not do variable selection like LASSO does as it doesn't set weights straight to 0. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 v/s L2\n",
    "\n",
    "The penalty for L1 takes the absolute value of the weights, which for a two parameter simple system results in a diamong shape plot as shown below. These diamond shape leads to more intersection with the axis, which explains why L1 tends to move some weights to 0 resulting in sparcity. \n",
    "\n",
    "On the other hand, L2 adds the sum of squared weight, leading to a circular shape instead. Unlike the diamong shaped L1, this moves all weights gradually towards 0 without setting them to 0, making L2 a more gentle and well-rounded approach. \n",
    "\n",
    "<img src=\"l1l2.png\" width=\"500\"/>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
